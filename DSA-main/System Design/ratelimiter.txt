Token Bucket:
                                        Refiller
                                            |
                                            |
                                        Bucket
                        Request------>Enough Token------->Yes forward
                                            |
                                        Drop Request
Leaking Bucket
                                        Request  
                                            |
                                        Bucket Full--->queue--->processed at fixed rate
                                            |
                                            yes drop request 
High Level
                    
                                              Cache<---------Workers-->Rules
                                            /
                    Client-------->Rate limiter middleware-------Limit not reached--->API Servers 
                          |                 |   \
                          |                 |    \(fetch counter from redis)
                           -----------------|     Redis
                           429 too many request

            
Network system rate limiter used to control rate by client or service. Http world rate limiter limit number of client over period.
    No more than 2post/sec.
    10 account/day from same IP.
    Reward no more than 5/week.
    Benefits:
        Prevent resource starvation caused by DOS(Denial of service)
            Twitter limit tweets 300 per/3hour
            Google Docs read request 300/60 sec
        Reduce Cost:
            Limiting number of calls reduce cost. 
            For third party APIs: check credit, make payment, retrieve health records.
        Prevent servers from overloaded: 
            Filter out excess request caused by bots or users.

Understand problem and Design scope:
    Client side or Server side--->Server side rate limiter.
    Throttle API request Basis: IP, User ID, Properties.
    Scale of system: Large number of requests.
    Distributed rate limiting Environment: Yes 
    Informed users who are throttled: Yes 
    Low Latency: Should not slow HTTP response time.
    Less memory as possible. Exception Handling. High fault tolerance.

Propose High Level Design and Get Buy in:
    Where to put Rate Limiter:
        Client Side Implementation: 
            Easily forged by malicious actors. We might not have control over client Implementation.
        Server Side Implementation:
            Client-------------------->(API Servers(Rate Limiter))
        Middleware Rate Limiter:
            We create a middleware rate limiter, which throttle request to your API.
            Client------->Rate Limiter------------->API Servers
            Rate Limiter throttle extra request and return 429 indicates too many request.
    Cloud Microservices:
        In cloud rate limiting is implemented using API gateway.
        Fully manage service support rate limiting, SSL termination, authentication, IP whitelisting, servicing static content.
        Where to implement rate limiter:
            Server side or Gateway: Depends on company current stack.
                Evaluate current stack such as programming language and cache. Make it efficient to implement rate limiting on server.
                Algorithm fits your business need, Server side full control third party gateway limited control.
                If already using Gateway for IP whitelisting, authentication you may add rate_limiter also.
                Building rate limiter service take time. If lack of engineering resource then commercial gateway is better option.
                
Algorithms:
    Token Bucket:
        Widely used, Used by internet companies. Both Amazon and Stripe used this.
        Bucket has pre-defined capacity. Tokens are put at present rate periodically. Once bucket is full no more token added.
        Each request consume one token, we check if enough token.
            If enough token we take one token and request goes through.
            If not enough token request dropped.
            Bucket size 4, Refill rate 4/1min.
            token_bucket(bucket_size, refill_rate): #maximum_token and token in bucket every sec.
        How many bucket:
            Different bucket for different API endpoint. User make 1post/seec, Add 150friend/day, 5post/sec. 3bucket are required.
            Throttle request based on IP address, each require bucket.
            System allow maximum of 10000request/sec then global bucket.
        Pros:
            Easy to implement. Memory efficient. 
            Allow burst of traffic for short period. Go as long as token left.
        Cons:
            Challenging input parameters to tune bucket_size and refill_rate.
    Leaking Bucket:
        Similar to token bucket request are processed at fixed rate. Implemented in FIFO. Use in Shopify
        Request arrive system check if queue is full. If not then request added. Other request dropped.
        Request pulled from queue and processed at regular intervals.
        leak_bucket(bucket_size, outflow_rate): Maximum token and token process at fixed rate.
        Pros:
            Memory efficient. Limited queue size. Suitable for stable outflow rate.
        Cons:
            Difficult to tune params. Burst of traffic fill queue and new request rate limited. 
    Fixed Window counter:
        Divide timeline into fixed sized window and counter for each window.Request increment counter by one.
        Once counter reach pre-defined threshold, new request drop until new window start.
        Between 2:00 and 2:01 five, 2:01 and 2:02 more five. But possible 2:005 and 2:015 ten request.
        Pros:
            Memory efficient. Resetting available quota at unit time fixed certain use case.
        Cons:
            Spike in traffic at edges allowed more request than quota.
    Sliding window log algorithm:
        Algorithm solved fixed counter issue. It keep tracks of request timestamp.Kept in cache such as sorted redis.
        When new request came remove older timestamp older than current window.
        Add timestamp of new request to log. If log size is lower or same than allow count then accept or reject.
        Pros: Rate limiting by this accurate. IN rolling window request will not exceed rate limit.
        Cons: Consume lot of memory because request rejected timestamp might still in memory.
    Sliding Window Counter:
        Hybrid approach combine fixed window counter and sliding window. Rolling min 70% and 30%.
        Formuala: request in current window(3) + request in previous window(5) * overlap percentage(70%)
        3 + 5*70% = 6.5 round to 6 
        Pros: Memory efficent. Smooth spikes in traffic because rate based on average rate of previous window.
        Cons: Works on not-so-strict look back window. Approx of actual rate because it assumes request in previous window evenly distributed.
              Cloudflare only 0.003% request wrongly allow rate limited 400 million requests.

High Level Architecture:
    Rate limit algorithm is simple. High level need a counter how many request sent from same user, IP address. If counter large request disallow.
    Using DB is slow due to disk access. In memory cache is fast and support time expiration. Redis is popular for rate limiting.
    It is in memory store offer two command: INCR(increase count by 1) and EXPIRE(set timeout expire automatically deleted)

Design Deep Dive:
    Rate limiting rules and stored?
        Lyft open source their rate limiting component.
            domain: messaging
            descriptors:
                -key: message_type
                Value: marketing
                rate_limit:
                    unit: day
                    request_per_unit: 5
    Handle request that rate limited 
        Rate limited HTTP response code 429(too many requests)
        HTTp response header take care of allowed remaining request before being throttled.
            X-Ratelimit-remaining: Remaining number of request per window.
            X-Ratelimit-Limit: How many call per window.
            X-Ratelimit-Retry-After: Number of sec to wait making request again without throttled.
        Rules stored on disk. Workers pull from disk and store in cache.
        Client sent to server but go through middleware first. Middleware fetch from cache(counter, last request timestamp)
    Rate limiter in Distributed System:
        To support multiple servers and concurrent thread:
        Challenges: 
            Race Condition:
                Can happen in highly concurrent environment. Two request simultaneously make counter from 3-->4 but it has to 5.
                A possible solution is Lock but it significantly slow down system.
                Other Lua Script and sorted set data structures in redis.
            Synchronization Issue:
                Another important factor in distributed system. For million of user one rate limiter server not enough to handle traffic.
                As web tier is stateless, client send request to different rate limiter. Without Synchronization rate limiter 1 not known about client 2.
                Solution:
                    Use sticky session that allow client to send traffic to same rate limiter.
                    Not advisable because of not scalable and flexible.Better to use centralized store like Redis.
    Performnce Optimization:
        Multi-Data center:
            Latency is high for user far from data centers. Most cloud service provider build many edge server location sround world.
            By 2020 cloudflare has 194 geographically distributed. Traffic is automatically routed to closest edge server.
        Synchronize data with eventual consistency model.
    Monitoring:
        Rate limiter algorithm and rules are effective.
            If rules are too restrict many valid request dropped.
            Rate limiter been ineffective in traffic like flash sales. We can use token bucket.

Wrap Up:
    Hard and Soft Limiting 
        Hard: Can't exceed threshold.
        Soft: Can exceed for a short period.
    Rate limiting different levels:
        HTTP: 7th level, By IP address using IPTables(IP:layer 3), 
        OSI Models layers(Open System Interconnected)
        PDNTSPA: Physycal, Data , Network, Transport, Session, Presentation, Application


